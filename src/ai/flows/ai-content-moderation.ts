// This is an autogenerated file from Firebase Studio.
'use server';

/**
 * @fileOverview AI-powered content moderation tool that automatically detects and flags inappropriate content.
 *
 * - moderateContent - A function that handles the content moderation process.
 * - ModerateContentInput - The input type for the moderateContent function.
 * - ModerateContentOutput - The return type for the moderateContent function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const ModerateContentInputSchema = z.object({
  text: z
    .string() 
    .describe('The text content to be moderated, such as video descriptions or comments.'),
});
export type ModerateContentInput = z.infer<typeof ModerateContentInputSchema>;

const ModerateContentOutputSchema = z.object({
  isSafe: z
    .boolean()
    .describe('Whether the content is considered safe and appropriate (true) or not (false).'),
  flaggedCategories: z.array(
    z.enum([
      'HATE_SPEECH',
      'SEXUALLY_EXPLICIT',
      'HARASSMENT',
      'DANGEROUS_CONTENT',
      'CIVIC_INTEGRITY',
    ])
  ).describe('List of categories the content was flagged for, if any.'),
  reason: z
    .string()
    .describe('The reason why the content was flagged as inappropriate.'),
});

export type ModerateContentOutput = z.infer<typeof ModerateContentOutputSchema>;

export async function moderateContent(input: ModerateContentInput): Promise<ModerateContentOutput> {
  return moderateContentFlow(input);
}

const moderateContentPrompt = ai.definePrompt({
  name: 'moderateContentPrompt',
  input: {schema: ModerateContentInputSchema},
  output: {schema: ModerateContentOutputSchema},
  prompt: `You are an AI content moderation tool. Your task is to analyze the given text and determine if it contains any inappropriate content such as hate speech, sexually explicit material, harassment, dangerous content, or content related to civic integrity violations.

  Text to analyze: {{{text}}}

  Respond with a JSON object indicating whether the content is safe, any flagged categories, and a reason for the decision.
`,
  config: {
    safetySettings: [
      {
        category: 'HARM_CATEGORY_HATE_SPEECH',
        threshold: 'BLOCK_ONLY_HIGH',
      },
      {
        category: 'HARM_CATEGORY_DANGEROUS_CONTENT',
        threshold: 'BLOCK_NONE',
      },
      {
        category: 'HARM_CATEGORY_HARASSMENT',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      },
      {
        category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
        threshold: 'BLOCK_LOW_AND_ABOVE',
      },
      {
        category: 'HARM_CATEGORY_CIVIC_INTEGRITY',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      }
    ]
  }
});

const moderateContentFlow = ai.defineFlow(
  {
    name: 'moderateContentFlow',
    inputSchema: ModerateContentInputSchema,
    outputSchema: ModerateContentOutputSchema,
  },
  async input => {
    const {output} = await moderateContentPrompt(input);
    return output!;
  }
);



